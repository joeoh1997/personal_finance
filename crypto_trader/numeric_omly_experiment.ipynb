{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ffbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, columns):\n",
    "    encoder = OneHotEncoder(drop=None).fit(\n",
    "        df[columns].values\n",
    "    )\n",
    "    encodings = encoder.transform(df[columns]).toarray()\n",
    "    \n",
    "    encodings = pd.DataFrame(\n",
    "        data=encodings,\n",
    "        columns=['encoded_'+str(n) for n in range(encodings.shape[1])]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    return pd.concat(\n",
    "        [df, encodings],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "def add_scaled_vars(df, scale_columns, write_to_new_df=True):\n",
    "    min_maxes = []\n",
    "    write_df = pd.DataFrame() if write_to_new_df else df\n",
    "    \n",
    "    scale_columns = df.columns if scale_columns == 'all' else scale_columns\n",
    "    \n",
    "    for column in scale_columns:\n",
    "        max_val, min_val = df[column].max(), df[column].min()\n",
    "        write_df[column+'_scaled'] = (df[column] - min_val) / (max_val - min_val)\n",
    "        \n",
    "        min_maxes.append([max_val, min_val])\n",
    "        \n",
    "    return write_df, pd.DataFrame(\n",
    "        data=np.array(min_maxes).T, columns=scale_columns, index=['max', 'min']\n",
    "    )\n",
    "    \n",
    "    \n",
    "def plot_data(df, columns, ticker):\n",
    "    ticker = ticker+'_'\n",
    "    columns = [ticker+var for var in columns]\n",
    "    \n",
    "    plt = px.line(\n",
    "        df,\n",
    "        x='timestamp',\n",
    "        y=columns\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train(df, columns):\n",
    "    return df.loc[df['is_test'] == 0, columns], df.loc[df['is_test'] == 1, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/streams/XRPEUR/numeric/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258217a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "targets = pd.DataFrame()\n",
    "\n",
    "use_smooth_data = True # test preprocessed, smooth preprocessed, unprocessed & maybe smooth unprocessed\n",
    "\n",
    "paths = os.listdir(dir_path)\n",
    "\n",
    "if not use_smooth_data:\n",
    "    paths = [path for path in paths if 'smooth' not in path] \n",
    "\n",
    "else:\n",
    "    paths = [path for path in paths if 'smooth' in path or 'target' in path] \n",
    "\n",
    "    \n",
    "for path in paths:\n",
    "    df = pd.read_csv(dir_path+path, index_col=0)\n",
    "    \n",
    "    if 'data' in path:\n",
    "        data = data.append(df, ignore_index=True)\n",
    "        \n",
    "    elif 'targets' in path:\n",
    "        targets = targets.append(df, ignore_index=True)\n",
    "        \n",
    "        \n",
    "# remove data with nan targets\n",
    "non_nan_mask = ~np.isnan(targets['label'].values)\n",
    "targets = targets[non_nan_mask].reset_index(drop=True)\n",
    "data = data[non_nan_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf607b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero one scale data \n",
    "scaled_data, scaled_min_maxes = add_scaled_vars(\n",
    "    data, 'all', write_to_new_df=True\n",
    ")\n",
    "scaled_data['is_test'] = targets['is_test']\n",
    "\n",
    "# encode label\n",
    "targets = one_hot_encoding(targets, ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a82732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OVERMPING:\n",
    "# 0: 10076  rows , 1: 4345  rows, 2: 2149  rows, 3: 133 rows\n",
    "encoding_cols = targets.columns[targets.columns.str.contains('encoded')]\n",
    "                                \n",
    "masks = targets[encoding_cols].astype(bool)\n",
    "oversamps = [2, 1, 1, 5, 35, 3]\n",
    "\n",
    "oversamp_targets = pd.DataFrame()\n",
    "oversamp_data = pd.DataFrame()\n",
    "\n",
    "for i, oversamp in enumerate(oversamps):\n",
    "    mask = masks['encoded_'+str(i)].values\n",
    "    print(f\"Class {i} num rows = {len(targets.iloc[mask])}\")\n",
    "    \n",
    "    for n in range(oversamp):\n",
    "        #print(f'Adding {i}')\n",
    "        oversamp_targets = oversamp_targets.append(targets[mask], ignore_index=True)\n",
    "        oversamp_data = oversamp_data.append(scaled_data[mask], ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "print('After..')\n",
    "masks = oversamp_targets[encoding_cols].astype(bool)\n",
    "\n",
    "for i in range(len(encoding_cols)):\n",
    "    mask = masks['encoded_'+str(i)].values\n",
    "    print(f\"Class {i} num rows = {len(oversamp_targets.iloc[mask])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train, y_test = get_test_train(oversamp_targets, ['label'])\n",
    "\n",
    "X_train, X_test = get_test_train(\n",
    "    oversamp_data,\n",
    "    oversamp_data.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\"\"\"\n",
    "   Having errors trying to save model from notebook, rerun modeling for now.\n",
    "   Errors wont be present when experiment is converted to python codebase (out of notebook)\n",
    "\"\"\"\n",
    "\n",
    "rerun_modeling = True\n",
    "rerun_grid_search = False\n",
    "\n",
    "model_file_name = \"catboost\" \n",
    "\n",
    "best_params = {\n",
    "    'depth': 15, 'l2_leaf_reg': 35, 'learning_rate': 0.01\n",
    "}\n",
    "\n",
    "\n",
    "# find the best params for CatBoost \n",
    "if rerun_grid_search:\n",
    "    parameters = {\n",
    "        'depth': [3, 6, 12, 15],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'l2_leaf_reg': [2, 15, 25, 35]\n",
    "    }\n",
    "\n",
    "    catboost_regressor = CatBoostClassifier(loss_function='MultiClass', iterations=3000)\n",
    "\n",
    "    Grid_CBC = HalvingGridSearchCV(\n",
    "        estimator=catboost_regressor,\n",
    "        param_grid=parameters,\n",
    "        cv=5, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    search = Grid_CBC.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = search.best_params_\n",
    "    catboost_regressor = search.best_estimator_\n",
    "    \n",
    "    #catboost_regressor.save_model(model_file_name)\n",
    "    \n",
    "    \n",
    "# Train CatBoost if best params are known\n",
    "if rerun_modeling:\n",
    "    catboost_regressor = CatBoostClassifier(\n",
    "        loss_function='MultiClass',\n",
    "        iterations=3000,\n",
    "        **best_params\n",
    "    )\n",
    "    catboost_regressor.fit(X_train, y_train)\n",
    "    #catboost_regressor.save_model(model_file_name)\n",
    "    \n",
    "\n",
    "# else:\n",
    "#     catboost_regressor = CatBoostRegressor().load_model(model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test = get_test_train(\n",
    "#     oversamp_targets,\n",
    "#     list(oversamp_targets.columns[oversamp_targets.columns.str.contains('encoded')])\n",
    "# )\n",
    "\n",
    "y_train, y_test = get_test_train(oversamp_targets, ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class salesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item], self.y[item] \n",
    "    \n",
    "    \n",
    "class compDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p = np.random.randint(0, self.__len__(), 1)[0]\n",
    "        \n",
    "        return self.x[item], self.x[p], int(self.y[item] != self.y[p]), self.y[item] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e3f608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers_sizes\n",
    "    ):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        for _, (in_size, out_size, use_bn, activation, dropout) in layers_sizes.items():\n",
    "            self.layers.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "            if use_bn:\n",
    "                self.layers.append(nn.BatchNorm1d(out_size))\n",
    "                \n",
    "            if activation is not None:\n",
    "                self.layers.append(activation())\n",
    "                \n",
    "            if dropout and dropout > 0:\n",
    "                self.layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)\n",
    "    \n",
    "    \n",
    "class NN_split(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers_sizes\n",
    "    ):\n",
    "        super(NN_split, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _, (in_size, out_size, use_bn, activation, dropout) in layers_sizes.items():\n",
    "            sub_layer = []\n",
    "            sub_layer.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "            if use_bn:\n",
    "                sub_layer.append(nn.BatchNorm1d(out_size))\n",
    "                \n",
    "            if activation is not None:\n",
    "                sub_layer.append(activation())\n",
    "                \n",
    "            if dropout and dropout > 0:\n",
    "                sub_layer.append(nn.Dropout(p=dropout))\n",
    "\n",
    "            self.layers.append(nn.Sequential(*sub_layer))  \n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2d4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1        0.01333333 0.98       0.14333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.1       , 0.03666667, 2.08      , 0.68666667])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "l = np.array([0, 0, 1, 1])\n",
    "\n",
    "a = np.array([\n",
    "    [0.1, 0.1, 0.1],\n",
    "    [0.5, 0.5, 0.5],\n",
    "    [0.9, 0.9, 0.9],\n",
    "    [0.89, 0.87, 0.88]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0.51, 0.52, 0.49],\n",
    "    [0.89, 0.87, 0.88],\n",
    "    [0.02, 0, 0.05]\n",
    "])\n",
    "\n",
    "a_d = np.array([-2, -0.89, 1, 1.07])\n",
    "b_d = np.array([-1, -0.94, -1.1, 0.9])\n",
    "\n",
    "l0 = np.abs(l - np.mean(np.abs(a-b), axis=1))\n",
    "print(l0)\n",
    "\n",
    "#vec_diff = np.mean(np.abs(a-b), axis=1)\n",
    "#print(vec_diff, np.abs(a_d-b_d))\n",
    "#l1 = (1 - l)*np.abs(vec_diff - np.abs(a_d-b_d)) #+ l*np.maximum((np.abs(a_d)+np.abs(b_d))-vec_diff)\n",
    "#print(l1)\n",
    "\n",
    "np.abs((a_d - b_d) - np.mean(np.abs(a-b), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.nn import functional as f\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, model, optimiser, loss_function,\n",
    "        device, batch_size, epochs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_function = loss_function \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        with torch.no_grad(): # disable autograd engine (no backprop)\n",
    "            self.model.eval() # dropout, batchnorm in inference mode\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            counter = 0\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x, y)\n",
    "                valid_loss += loss.item()\n",
    "                counter += 1\n",
    "            valid_loss /= counter\n",
    "        self.model.train()\n",
    "\n",
    "        return valid_loss\n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        h = f.softmax(self.model.forward(x), dim=1)\n",
    "        return self.loss_function(h, y)#y.to(torch.long))\n",
    "    \n",
    "    def train(self, train_loader, val_loader, model_save_path):\n",
    "        best_valid_loss = np.inf\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        weights_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                self.optimiser.zero_grad()\n",
    "                \n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x, y)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "#                 weights_losses.append(\n",
    "#                     loss +\n",
    "#                     list(model.state_dict()['layers.4.0.weight'].cpu().detach().numpy().flatten())\n",
    "#                 )\n",
    "\n",
    "                loss.backward() # compute gradients\n",
    "\n",
    "                self.optimiser.step() # Update params based on gradients\n",
    "\n",
    "            valid_loss = self.validate(val_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            val_losses.append(valid_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch {} of {}'.format(epoch, self.epochs))\n",
    "                print(f\"\\tEpoch Loss={epoch_loss}, Validation Loss={valid_loss}\")\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f'\\tEpoch({epoch}), Val Loss({valid_loss}), Saving new best model')\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    model_save_path\n",
    "                )\n",
    "        return pd.DataFrame(data=np.array([train_losses, val_losses]).T, columns=['training_loss', 'vaidation_loss'])\n",
    "    \n",
    "    \n",
    "    \n",
    "class CompTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, model, optimiser, loss_function,\n",
    "        device, batch_size, epochs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_function = loss_function \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def compute_loss(self, x_0, x_1, y):\n",
    "        vec_0 = self.model.forward(x_0.to(self.device))\n",
    "        vec_1 = self.model.forward(x_1.to(self.device))\n",
    "        \n",
    "        loss = torch.abs(y - torch.mean(torch.abs(vec_0-vec_1), axis=1))\n",
    "        \n",
    "        #print(vec_0.shape, vec_1.shape, loss.shape)\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "    def get_vecs(self, train_loader, val_loader):\n",
    "        r = {'train': train_loader, 'val': val_loader}\n",
    "        vec_df = \n",
    "        \n",
    "        with torch.no_grad(): # disable autograd engine (no backprop)\n",
    "            self.model.eval() # dropout, batchnorm in inference mode\n",
    "            vecs = []\n",
    "        \n",
    "            for n, loader in r.items():\n",
    "                for x, _, _, cls  in loader:\n",
    "                    v = self.model.forward(x.to(self.device)).cpu().detach().numpy().flatten()\n",
    "                    vec_df = vec_df.append(\n",
    "                        {'v0': v[0], 'v1': v[1], 'v2': v[2], 'class': cls, 'set': n},\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "                    \n",
    "        self.model.train()\n",
    "        return vec_df\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        with torch.no_grad(): # disable autograd engine (no backprop)\n",
    "            self.model.eval() # dropout, batchnorm in inference mode\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            counter = 0\n",
    "            for x_0, x_1, y, _ in val_loader:\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x_0, x_1, y)\n",
    "                valid_loss += loss.item()\n",
    "                counter += 1\n",
    "            valid_loss /= counter\n",
    "        self.model.train()\n",
    "\n",
    "        return valid_loss\n",
    "    \n",
    "    def train(self, train_loader, val_loader, model_save_path):\n",
    "        best_valid_loss = np.inf\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        weights_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for x_0, x_1, y, _ in train_loader:\n",
    "                self.optimiser.zero_grad()\n",
    "                \n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x_0, x_1, y)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                loss.backward() # compute gradients\n",
    "\n",
    "                self.optimiser.step() # Update params based on gradients\n",
    "\n",
    "            valid_loss = self.validate(val_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            val_losses.append(valid_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch {} of {}'.format(epoch, self.epochs))\n",
    "                print(f\"\\tEpoch Loss={epoch_loss}, Validation Loss={valid_loss}\")\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f'\\tEpoch({epoch}), Val Loss({valid_loss}), Saving new best model')\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    model_save_path\n",
    "                )\n",
    "        return pd.DataFrame(data=np.array([train_losses, val_losses]).T, columns=['training_loss', 'vaidation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ce863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "    * GET val score by/per class\n",
    "    \n",
    "    * Visualize learning:\n",
    "        * visualize 2d weights & loss\n",
    "        * visualize 2d prediction values & see if classes are clustered\n",
    "            * maybe introduce some contrastive loss\n",
    "    \n",
    "    * Continue enembe research\n",
    "        * Try random search & train for 10-30 epochs\n",
    "        \n",
    "    * Impement param search\n",
    "    \n",
    "    * compre, processesd, unprocessesd & smooth\n",
    "        \n",
    "    * make more classes e.g. >1% change, >5% change\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "learning_rate = 0.0001\n",
    "weight_reg = 0.0001\n",
    "\n",
    "model_save_path = 'model/model.pth'\n",
    "losses_save_path = 'data/losses.csv'\n",
    "\n",
    "retrain = False #False, True\n",
    "load_pretrained_model = False \n",
    "\n",
    "layer_sizes = {\n",
    "    'layer_0': [X_train.shape[1], 125, False, nn.RReLU, 0.166],\n",
    "    'layer_1': [125, 500, False, nn.ELU, 0.33],\n",
    "    'layer_2': [500, 5, False, nn.GELU, 0],\n",
    "    'layer_3': [5, 33, False, nn.GELU, None], # RReLU, GELU\n",
    "    #'layer_4': [2, 1, None, nn.GELU, None],\n",
    "    'layer_5': [33, 3, False, nn.Tanh, None], # y_train.shape[1]\n",
    "}\n",
    "\n",
    "loss_function = nn.L1Loss() #nn.MSELoss() #nn.BCELoss() #nn.MSELoss() #nn.L1Loss()  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    compDataset(  #compDataset,  salesDataset\n",
    "        X_train.values.astype(np.float32),\n",
    "        y_train.values.astype(np.float32),\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    compDataset(\n",
    "        X_test.values.astype(np.float32),\n",
    "        y_test.values.astype(np.float32),\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "model = NN_split(layer_sizes).to(device)\n",
    "#print(model)\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_reg)\n",
    "\n",
    "trainer = CompTrainer(  # CompTrainer, Trainer\n",
    "    model, optimiser, loss_function, device, batch_size, epochs\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    if load_pretrained_model:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        \n",
    "    losses = trainer.train(train_loader, val_loader, model_save_path)\n",
    "    losses.to_csv(losses_save_path, index=False)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    losses = pd.read_csv(losses_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_vecs(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b22a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.state_dict()['layers.4.0.weight'].cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be163a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "weight_reg = 0.0001  #0.005\n",
    "model_2_save_path = 'model/model_2.pth'\n",
    "losses_2_save_path = 'data/losses_2.csv'\n",
    "\n",
    "retrain = False #False, True\n",
    "\n",
    "model_2 = NN(layer_sizes).to(device)\n",
    "\n",
    "optimiser_2 = optim.Adam(model_2.parameters(), lr=learning_rate, weight_decay=weight_reg)\n",
    "\n",
    "trainer_2 = Trainer(\n",
    "    model_2, optimiser_2, loss_function, device, batch_size, epochs\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    losses_2 = trainer_2.train(train_loader, val_loader, model_2_save_path)\n",
    "    losses_2.to_csv(losses_2_save_path, index=False)\n",
    "    \n",
    "else:\n",
    "    model_2.load_state_dict(torch.load(model_2_save_path))\n",
    "    losses_2 = pd.read_csv(losses_2_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try random search & train for 10-30 epochs\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "#epochs = 30\n",
    "weight_reg = 0.01\n",
    "join_weights = [\n",
    "    [0, 1], [1, 0],\n",
    "    [0.1, 0.9], [0.9, 0.1], \n",
    "    [0.2, 0.8], [0.2, 0.8], \n",
    "    [0.4, 0.6], [0.6, 0.4],\n",
    "    [0.5, 0.5]\n",
    "]\n",
    "\n",
    "model_join_save_path = 'model/model_join.pth'\n",
    "losses_join_save_path = 'data/losses_join.csv'\n",
    "\n",
    "\n",
    "model_join = NN(layer_sizes).to(device)\n",
    "optimiser_join = optim.Adam(model_join.parameters(), lr=learning_rate, weight_decay=weight_reg)\n",
    "\n",
    "#for _ in range(100):\n",
    "for join_weight in join_weights:\n",
    "    #print(join_weight)\n",
    "    join_state_dict = OrderedDict()\n",
    "    #r = np.clip(np.random.normal(0.5, scale=0.4, size=1), 0, 1)[0]\n",
    "    #print(\"Split: \", r, r-1)\n",
    "    print(\"Split: \", join_weight[0], join_weight[1])\n",
    "\n",
    "    for key_0, val_0 in torch.load(model_save_path).items():\n",
    "        for key_1, val_1 in torch.load(model_2_save_path).items():\n",
    "            if key_0 == key_1:\n",
    "                #join_state_dict[key_0] = val_0*r + val_1*(1-r)\n",
    "                join_state_dict[key_0] = val_0*join_weight[0] + val_1*join_weight[1]\n",
    "\n",
    "\n",
    "    model_join.load_state_dict(join_state_dict)    \n",
    "\n",
    "    trainer_join = Trainer(\n",
    "        model_join, optimiser_join, loss_function, device, batch_size, epochs\n",
    "    )\n",
    "\n",
    "    val_loss = trainer_join.validate(val_loader)\n",
    "    print(\"Val Score:\", val_loss, \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "join_weight = [0.5, 0.5]\n",
    "join_state_dict = OrderedDict()\n",
    "\n",
    "for key_0, val_0 in torch.load(model_save_path).items():\n",
    "    for key_1, val_1 in torch.load(model_2_save_path).items():\n",
    "        if key_0 == key_1:\n",
    "            join_state_dict[key_0] = val_0*join_weight[0] + val_1*join_weight[1]\n",
    "\n",
    "\n",
    "model_join.load_state_dict(join_state_dict)    \n",
    "\n",
    "trainer_join = Trainer(\n",
    "    model_join, optimiser_join, loss_function, device, batch_size, epochs\n",
    ")\n",
    "\n",
    "val_loss = trainer_join.validate(val_loader)\n",
    "print(\"Val Score:\", val_loss, \"\\n\")\n",
    "\n",
    "losses_join = trainer_join.train(train_loader, val_loader, model_join_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot taining losses\n",
    "fig = px.line(\n",
    "    losses,\n",
    "    x=losses.index, \n",
    "    y=\"training_loss\", \n",
    "    title=\"Neural Net Training Losses\",\n",
    "    #color='is_test'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    losses,\n",
    "    x=losses.index, \n",
    "    y=\"vaidation_loss\", \n",
    "    title=\"Neural Net Validation Losses\",\n",
    "    #color='is_test'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026195b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupped_plot(group):\n",
    "    fig = px.scatter(\n",
    "        group,\n",
    "        x=\"timestamp\", \n",
    "        y=[\"is_test\", \"pred\", \"class\"], \n",
    "    )\n",
    "    fig.show() \n",
    "\n",
    "def plot_predictions(df, predictions):\n",
    "    df['pred']  = np.argmax(predictions, axis=1) \n",
    "    df['class'] = np.argmax(df[df.columns[df.columns.str.contains('encoded')]].values, axis=1)  \n",
    "    \n",
    "    display(df)\n",
    "    df.groupby(['class']).apply(groupped_plot)\n",
    "    \n",
    "    \n",
    "def plot_predictions_2(df, predictions): \n",
    "    #df['is_test_mod'] = df['is_test']*4 -2\n",
    "    \n",
    "    for i, col in enumerate(df.columns[df.columns.str.contains('encoded')]):\n",
    "        df['pred'] = predictions[:, i]\n",
    "        fig = px.scatter(\n",
    "            df,\n",
    "            x=df.index, \n",
    "            y=[col, 'pred'],#, 'is_test_mod'], \n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b689dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display Model Predictions\n",
    "\n",
    "potting_model = model # model, model_2, model_join\n",
    "\n",
    "\n",
    "potting_model.eval()\n",
    "all_data = torch.Tensor(scaled_data[scaled_data.columns].values.astype(np.float32)).to(device)\n",
    "predictions = f.softmax(potting_model.forward(all_data), dim=1).cpu().detach().numpy()\n",
    "\n",
    "mask = targets['is_test'] == 1\n",
    "\n",
    "plot_predictions_2(\n",
    "    targets.loc[mask].reset_index(drop=True),\n",
    "    predictions[mask]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "potting_model = model # model, model_2, model_join\n",
    "\n",
    "\n",
    "potting_model.eval()\n",
    "all_data = torch.Tensor(scaled_data[scaled_data.columns].values.astype(np.float32)).to(device)\n",
    "vec = pd.DataFrame(\n",
    "    columns=['v0', 'v1', 'v2'],\n",
    "    data=potting_model.forward(all_data).cpu().detach().numpy()\n",
    ")\n",
    "vec['is_test'] = targets['is_test']\n",
    "vec['label'] = targets['label']\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(vec[vec['is_test'] == 1], x='v0', y='v1', z='v2',\n",
    "              color='label')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(vec[vec['is_test'] == 0], x='v0', y='v1', z='v2',\n",
    "              color='label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 300, True, nn.RReLU],\n",
    "#     'layer_1': [300, 100, True, nn.RReLU],\n",
    "#     'layer_2': [100, 7, True, nn.RReLU],\n",
    "#     'layer_3': [7, y_train.shape[1], False, None],\n",
    "# }\n",
    "\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 100, True, nn.RReLU, 0],\n",
    "#     'layer_1': [100, 300, False, nn.RReLU, 0.33],\n",
    "#     'layer_2': [300, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, y_train.shape[1], False, None, None],\n",
    "# } nn.L1Loss()\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 133, True, nn.Tanh, 0],\n",
    "#     'layer_1': [133, 175, True, nn.RReLU, 0.15],\n",
    "#     'layer_2': [175, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, y_train.shape[1], False, None, None],\n",
    "# } nn.L1Loss()\n",
    "\n",
    "\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 66, True, nn.Tanh, 0],\n",
    "#     'layer_1': [66, 175, True, nn.RReLU, 0.15],\n",
    "#     'layer_2': [175, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, 2, True, nn.RReLU, None],\n",
    "#     'layer_4': [2, 15, True, nn.RReLU, None],\n",
    "#     'layer_5': [15, y_train.shape[1], False, None, None],\n",
    "# }\n",
    "\n",
    "\n",
    "# for param, val in model.state_dict().items():\n",
    "#     print(param, val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
