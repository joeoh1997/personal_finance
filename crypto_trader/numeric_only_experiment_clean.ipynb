{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ffbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, columns):\n",
    "    encoder = OneHotEncoder(drop=None).fit(\n",
    "        df[columns].values\n",
    "    )\n",
    "    encodings = encoder.transform(df[columns]).toarray()\n",
    "    \n",
    "    encodings = pd.DataFrame(\n",
    "        data=encodings,\n",
    "        columns=['encoded_'+str(n) for n in range(encodings.shape[1])]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    return pd.concat(\n",
    "        [df, encodings],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "def add_scaled_vars(df, scale_columns, write_to_new_df=True):\n",
    "    min_maxes = []\n",
    "    write_df = pd.DataFrame() if write_to_new_df else df\n",
    "    \n",
    "    scale_columns = df.columns if scale_columns == 'all' else scale_columns\n",
    "    \n",
    "    for column in scale_columns:\n",
    "        max_val, min_val = df[column].max(), df[column].min()\n",
    "        write_df[column+'_scaled'] = (df[column] - min_val) / (max_val - min_val)\n",
    "        \n",
    "        min_maxes.append([max_val, min_val])\n",
    "        \n",
    "    return write_df, pd.DataFrame(\n",
    "        data=np.array(min_maxes).T, columns=scale_columns, index=['max', 'min']\n",
    "    )\n",
    "    \n",
    "    \n",
    "def plot_data(df, columns, ticker):\n",
    "    ticker = ticker+'_'\n",
    "    columns = [ticker+var for var in columns]\n",
    "    \n",
    "    plt = px.line(\n",
    "        df,\n",
    "        x='timestamp',\n",
    "        y=columns\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train(df, columns):\n",
    "    return df.loc[df['is_test'] == 0, columns], df.loc[df['is_test'] == 1, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/streams/XRPEUR/numeric/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258217a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "targets = pd.DataFrame()\n",
    "\n",
    "use_smooth_data = False # test preprocessed, smooth preprocessed, unprocessed & maybe smooth unprocessed\n",
    "\n",
    "paths = os.listdir(dir_path)\n",
    "\n",
    "if not use_smooth_data:\n",
    "    paths = [path for path in paths if 'smooth' not in path] \n",
    "\n",
    "else:\n",
    "    paths = [path for path in paths if 'smooth' in path or 'target' in path] \n",
    "\n",
    "    \n",
    "for path in paths:\n",
    "    df = pd.read_csv(dir_path+path, index_col=0)\n",
    "    \n",
    "    if 'data' in path:\n",
    "        data = data.append(df, ignore_index=True)\n",
    "        \n",
    "    elif 'targets' in path:\n",
    "        targets = targets.append(df, ignore_index=True)\n",
    "        \n",
    "        \n",
    "# remove data with nan targets\n",
    "non_nan_mask = ~np.isnan(targets['label'].values)\n",
    "targets = targets[non_nan_mask].reset_index(drop=True)\n",
    "data = data[non_nan_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf607b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero one scale data \n",
    "scaled_data, scaled_min_maxes = add_scaled_vars(\n",
    "    data, 'all', write_to_new_df=True\n",
    ")\n",
    "scaled_data['is_test'] = targets['is_test']\n",
    "\n",
    "# encode label\n",
    "targets = one_hot_encoding(targets, ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a82732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OVERMPING:\n",
    "# 0: 10076  rows , 1: 4345  rows, 2: 2149  rows, 3: 133 rows\n",
    "encoding_cols = targets.columns[targets.columns.str.contains('encoded')]\n",
    "                                \n",
    "masks = targets[encoding_cols].astype(bool)\n",
    "oversamps = [2, 1, 1, 5, 35, 3]\n",
    "\n",
    "oversamp_targets = pd.DataFrame()\n",
    "oversamp_data = pd.DataFrame()\n",
    "\n",
    "for i, oversamp in enumerate(oversamps):\n",
    "    mask = masks['encoded_'+str(i)].values\n",
    "    print(f\"Class {i} num rows = {len(targets.iloc[mask])}\")\n",
    "    \n",
    "    for n in range(oversamp):\n",
    "        #print(f'Adding {i}')\n",
    "        oversamp_targets = oversamp_targets.append(targets[mask], ignore_index=True)\n",
    "        oversamp_data = oversamp_data.append(scaled_data[mask], ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "print('After..')\n",
    "masks = oversamp_targets[encoding_cols].astype(bool)\n",
    "\n",
    "for i in range(len(encoding_cols)):\n",
    "    mask = masks['encoded_'+str(i)].values\n",
    "    print(f\"Class {i} num rows = {len(oversamp_targets.iloc[mask])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test = get_test_train(\n",
    "#     oversamp_targets,\n",
    "#     list(oversamp_targets.columns[oversamp_targets.columns.str.contains('encoded')])\n",
    "# )\n",
    "\n",
    "y_train, y_test = get_test_train(oversamp_targets, ['label'])\n",
    "\n",
    "X_train, X_test = get_test_train(\n",
    "    oversamp_data,\n",
    "    oversamp_data.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class salesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item], self.y[item] \n",
    "    \n",
    "    \n",
    "class compDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p = np.random.randint(0, self.__len__(), 1)[0]\n",
    "        \n",
    "        return self.x[item], self.x[p], int(self.y[item] != self.y[p]), self.y[item] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers_sizes\n",
    "    ):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        for _, (in_size, out_size, use_bn, activation, dropout) in layers_sizes.items():\n",
    "            self.layers.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "            if use_bn:\n",
    "                self.layers.append(nn.BatchNorm1d(out_size))\n",
    "                \n",
    "            if activation is not None:\n",
    "                self.layers.append(activation())\n",
    "                \n",
    "            if dropout and dropout > 0:\n",
    "                self.layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)\n",
    "    \n",
    "    \n",
    "class NN_split(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers_sizes\n",
    "    ):\n",
    "        super(NN_split, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _, (in_size, out_size, use_bn, activation, dropout) in layers_sizes.items():\n",
    "            sub_layer = []\n",
    "            sub_layer.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "            if use_bn:\n",
    "                sub_layer.append(nn.BatchNorm1d(out_size))\n",
    "                \n",
    "            if activation is not None:\n",
    "                sub_layer.append(activation())\n",
    "                \n",
    "            if dropout and dropout > 0:\n",
    "                sub_layer.append(nn.Dropout(p=dropout))\n",
    "\n",
    "            self.layers.append(nn.Sequential(*sub_layer))  \n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.nn import functional as f\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, model, optimiser, loss_function,\n",
    "        device, batch_size, epochs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_function = loss_function \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        with torch.no_grad(): # disable autograd engine (no backprop)\n",
    "            self.model.eval() # dropout, batchnorm in inference mode\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            counter = 0\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x, y)\n",
    "                valid_loss += loss.item()\n",
    "                counter += 1\n",
    "            valid_loss /= counter\n",
    "        self.model.train()\n",
    "\n",
    "        return valid_loss\n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        h = f.softmax(self.model.forward(x), dim=1)\n",
    "        return self.loss_function(h, y)#y.to(torch.long))\n",
    "    \n",
    "    def train(self, train_loader, val_loader, model_save_path):\n",
    "        best_valid_loss = np.inf\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        weights_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                self.optimiser.zero_grad()\n",
    "                \n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x, y)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "#                 weights_losses.append(\n",
    "#                     loss +\n",
    "#                     list(model.state_dict()['layers.4.0.weight'].cpu().detach().numpy().flatten())\n",
    "#                 )\n",
    "\n",
    "                loss.backward() # compute gradients\n",
    "\n",
    "                self.optimiser.step() # Update params based on gradients\n",
    "\n",
    "            valid_loss = self.validate(val_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            val_losses.append(valid_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch {} of {}'.format(epoch, self.epochs))\n",
    "                print(f\"\\tEpoch Loss={epoch_loss}, Validation Loss={valid_loss}\")\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f'\\tEpoch({epoch}), Val Loss({valid_loss}), Saving new best model')\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    model_save_path\n",
    "                )\n",
    "        return pd.DataFrame(data=np.array([train_losses, val_losses]).T, columns=['training_loss', 'vaidation_loss'])\n",
    "    \n",
    "    \n",
    "    \n",
    "class CompTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, model, optimiser, loss_function,\n",
    "        device, batch_size, epochs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_function = loss_function \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def compute_loss(self, x_0, x_1, y):\n",
    "        vec_0 = self.model.forward(x_0.to(self.device))\n",
    "        vec_1 = self.model.forward(x_1.to(self.device))\n",
    "        \n",
    "        loss = torch.abs(y - torch.mean(torch.abs(vec_0-vec_1), axis=1))\n",
    "        \n",
    "        #print(vec_0.shape, vec_1.shape, loss.shape)\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "    def vv(self):\n",
    "        self.model.eval()\n",
    "        all_data = torch.Tensor(scaled_data[scaled_data.columns].values.astype(np.float32)).to(device)\n",
    "        vec = pd.DataFrame(\n",
    "            columns=['v0', 'v1', 'v2'],\n",
    "            data=self.model.forward(all_data).cpu().detach().numpy()\n",
    "        )\n",
    "        vec['is_test'] = ['test'] * len(targets)\n",
    "        vec.loc[targets['is_test'] == 0, 'is_test'] = 'train'\n",
    "\n",
    "        vec['label'] = vec['is_test'] + \"_\" + targets['label'].astype(int).astype(str)\n",
    "\n",
    "        fig = px.scatter_3d(vec, x='v0', y='v1', z='v2',\n",
    "                      color='label')\n",
    "        fig.show()\n",
    "        self.model.train()\n",
    "        \n",
    "        \n",
    "    def validate(self, val_loader):\n",
    "        with torch.no_grad(): # disable autograd engine (no backprop)\n",
    "            self.model.eval() # dropout, batchnorm in inference mode\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            counter = 0\n",
    "            for x_0, x_1, y, _ in val_loader:\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x_0, x_1, y)\n",
    "                valid_loss += loss.item()\n",
    "                counter += 1\n",
    "            valid_loss /= counter\n",
    "        self.model.train()\n",
    "\n",
    "        return valid_loss\n",
    "    \n",
    "    def train(self, train_loader, val_loader, model_save_path):\n",
    "        best_valid_loss = np.inf\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        weights_losses = []\n",
    "        self.vv()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for x_0, x_1, y, _ in train_loader:\n",
    "                self.optimiser.zero_grad()\n",
    "                \n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                loss = self.compute_loss(x_0, x_1, y)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                loss.backward() # compute gradients\n",
    "\n",
    "                self.optimiser.step() # Update params based on gradients\n",
    "\n",
    "            valid_loss = self.validate(val_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            val_losses.append(valid_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch {} of {}'.format(epoch, self.epochs))\n",
    "                print(f\"\\tEpoch Loss={epoch_loss}, Validation Loss={valid_loss}\")\n",
    "                self.vv()\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f'\\tEpoch({epoch}), Val Loss({valid_loss}), Saving new best model')\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    model_save_path\n",
    "                )\n",
    "        return pd.DataFrame(data=np.array([train_losses, val_losses]).T, columns=['training_loss', 'vaidation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ce863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "    * GET val score by/per class\n",
    "    \n",
    "    * Visualize learning:\n",
    "        * visualize 2d weights & loss\n",
    "        * visualize 2d prediction values & see if classes are clustered\n",
    "            * maybe introduce some contrastive loss\n",
    "    \n",
    "    * Continue enembe research\n",
    "        * Try random search & train for 10-30 epochs\n",
    "        \n",
    "    * Impement param search\n",
    "    \n",
    "    * compre, processesd, unprocessesd & smooth\n",
    "        \n",
    "    * make more classes e.g. >1% change, >5% change\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "learning_rate = 0.0005\n",
    "weight_reg = 0.001\n",
    "\n",
    "model_save_path = 'model/model.pth'\n",
    "losses_save_path = 'data/losses.csv'\n",
    "\n",
    "retrain = True #False, True\n",
    "load_pretrained_model = True \n",
    "\n",
    "layer_sizes = {\n",
    "    'layer_0': [X_train.shape[1], 300, True, nn.GELU, 0],\n",
    "    'layer_1': [300, 150, True, nn.GELU, 0],\n",
    "    'layer_5': [150, 3, False, nn.GELU, 0], # y_train.shape[1]\n",
    "}\n",
    "\n",
    "loss_function = nn.MSELoss() #nn.MSELoss() #nn.BCELoss() #nn.MSELoss() #nn.L1Loss()  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    compDataset(  #compDataset,  salesDataset\n",
    "        X_train.values.astype(np.float32),\n",
    "        y_train.values.astype(np.float32),\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    compDataset(\n",
    "        X_test.values.astype(np.float32),\n",
    "        y_test.values.astype(np.float32),\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "model = NN_split(layer_sizes).to(device)\n",
    "#print(model)\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_reg)\n",
    "\n",
    "trainer = CompTrainer(  # CompTrainer, Trainer\n",
    "    model, optimiser, loss_function, device, batch_size, epochs\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    if load_pretrained_model:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        \n",
    "    losses = trainer.train(train_loader, val_loader, model_save_path)\n",
    "    losses.to_csv(losses_save_path, index=False)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    losses = pd.read_csv(losses_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot taining losses\n",
    "fig = px.line(\n",
    "    losses,\n",
    "    x=losses.index, \n",
    "    y=\"training_loss\", \n",
    "    title=\"Neural Net Training Losses\",\n",
    "    #color='is_test'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    losses,\n",
    "    x=losses.index, \n",
    "    y=\"vaidation_loss\", \n",
    "    title=\"Neural Net Validation Losses\",\n",
    "    #color='is_test'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026195b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupped_plot(group):\n",
    "    fig = px.scatter(\n",
    "        group,\n",
    "        x=\"timestamp\", \n",
    "        y=[\"is_test\", \"pred\", \"class\"], \n",
    "    )\n",
    "    fig.show() \n",
    "\n",
    "def plot_predictions(df, predictions):\n",
    "    df['pred']  = np.argmax(predictions, axis=1) \n",
    "    df['class'] = np.argmax(df[df.columns[df.columns.str.contains('encoded')]].values, axis=1)  \n",
    "    \n",
    "    display(df)\n",
    "    df.groupby(['class']).apply(groupped_plot)\n",
    "    \n",
    "    \n",
    "def plot_predictions_2(df, predictions): \n",
    "    #df['is_test_mod'] = df['is_test']*4 -2\n",
    "    \n",
    "    for i, col in enumerate(df.columns[df.columns.str.contains('encoded')]):\n",
    "        df['pred'] = predictions[:, i]\n",
    "        fig = px.scatter(\n",
    "            df,\n",
    "            x=df.index, \n",
    "            y=[col, 'pred'],#, 'is_test_mod'], \n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b689dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display Model Predictions\n",
    "\n",
    "potting_model = model # model, model_2, model_join\n",
    "\n",
    "\n",
    "potting_model.eval()\n",
    "all_data = torch.Tensor(scaled_data[scaled_data.columns].values.astype(np.float32)).to(device)\n",
    "predictions = f.softmax(potting_model.forward(all_data), dim=1).cpu().detach().numpy()\n",
    "\n",
    "mask = targets['is_test'] == 1\n",
    "\n",
    "plot_predictions_2(\n",
    "    targets.loc[mask].reset_index(drop=True),\n",
    "    predictions[mask]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf27d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "potting_model = model # model, model_2, model_join\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 300, True, nn.RReLU],\n",
    "#     'layer_1': [300, 100, True, nn.RReLU],\n",
    "#     'layer_2': [100, 7, True, nn.RReLU],\n",
    "#     'layer_3': [7, y_train.shape[1], False, None],\n",
    "# }\n",
    "\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 100, True, nn.RReLU, 0],\n",
    "#     'layer_1': [100, 300, False, nn.RReLU, 0.33],\n",
    "#     'layer_2': [300, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, y_train.shape[1], False, None, None],\n",
    "# } nn.L1Loss()\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 133, True, nn.Tanh, 0],\n",
    "#     'layer_1': [133, 175, True, nn.RReLU, 0.15],\n",
    "#     'layer_2': [175, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, y_train.shape[1], False, None, None],\n",
    "# } nn.L1Loss()\n",
    "\n",
    "\n",
    "\n",
    "# layer_sizes = {\n",
    "#     'layer_0': [X_train.shape[1], 66, True, nn.Tanh, 0],\n",
    "#     'layer_1': [66, 175, True, nn.RReLU, 0.15],\n",
    "#     'layer_2': [175, 25, True, nn.RReLU, None],\n",
    "#     'layer_3': [25, 2, True, nn.RReLU, None],\n",
    "#     'layer_4': [2, 15, True, nn.RReLU, None],\n",
    "#     'layer_5': [15, y_train.shape[1], False, None, None],\n",
    "# }\n",
    "\n",
    "\n",
    "# for param, val in model.state_dict().items():\n",
    "#     print(param, val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
