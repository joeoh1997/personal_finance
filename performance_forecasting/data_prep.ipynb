{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from performance_forecasting.data_creator import download_statement_data_for_exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddce23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "def hash_encode_gt_features(\n",
    "        df, columns, concat=True, num_encode_dimensions=32, hashing_method='md5'\n",
    "):\n",
    "    hash_encoding = ce.HashingEncoder.hashing_trick(\n",
    "        df[columns], hashing_method=hashing_method, \n",
    "        N=num_encode_dimensions, cols=columns\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        pd.concat([df, hash_encoding],axis=1), \n",
    "        [\"col_\"+str(i) for i in range(num_encode_dimensions)]\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "def one_hot_encoding(df, columns, concat=True):\n",
    "    encoder = OneHotEncoder(drop=None).fit(\n",
    "        df[columns].values\n",
    "    )\n",
    "    encodings = encoder.transform(df[columns]).toarray()\n",
    "    \n",
    "    encodings = pd.DataFrame(\n",
    "        data=encodings,\n",
    "        columns=['encoded_'+str(n) for n in range(encodings.shape[1])]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    if concat:\n",
    "        return pd.concat(\n",
    "            [df, encodings],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        return encodings\n",
    "\n",
    "\n",
    "def add_scaled_vars(df, scale_columns, write_to_new_df=True, prefix='_scaled'):\n",
    "    min_maxes = []\n",
    "    write_df = pd.DataFrame() if write_to_new_df else df\n",
    "    prefix = '' if write_to_new_df else prefix\n",
    "    \n",
    "    scale_columns = df.columns if scale_columns == 'all' else scale_columns\n",
    "    \n",
    "    for column in scale_columns:\n",
    "        max_val, min_val = df[column].max(), df[column].min()\n",
    "        write_df[column+prefix] = (df[column] - min_val) / (max_val - min_val)\n",
    "        \n",
    "        min_maxes.append([max_val, min_val])\n",
    "        \n",
    "    return write_df, pd.DataFrame(\n",
    "        data=np.array(min_maxes).T, columns=scale_columns, index=['max', 'min']\n",
    "    )\n",
    "\n",
    "\n",
    "def _inverse_log(var, neg=False):\n",
    "    a = 1 if neg else -1\n",
    "    b = -1*a\n",
    "    return a * np.log(b/var)\n",
    "\n",
    "def _sqrt(var, neg=False):\n",
    "    a = -1 if neg else 1\n",
    "    return a * np.sqrt(a*var)\n",
    "\n",
    "def math_func_scaling(\n",
    "    df, \n",
    "    scale_columns, \n",
    "    prefix='_log_scaled', \n",
    "    write_to_new_df=True,\n",
    "    math_func='log'\n",
    "):\n",
    "    if math_func == 'log':\n",
    "        math_func = _inverse_log\n",
    "    else:\n",
    "        math_func = _sqrt\n",
    "\n",
    "    write_df = pd.DataFrame() if write_to_new_df else df\n",
    "    prefix = '' if write_to_new_df else prefix\n",
    "    \n",
    "    maxes = pd.DataFrame()\n",
    "    \n",
    "    for column in scale_columns:\n",
    "        zero_mask = df[column] == 0\n",
    "        negative_mask = df[column] < 0\n",
    "        new_column = column+prefix\n",
    "        \n",
    "        write_df[new_column] = math_func(df[column])\n",
    "        \n",
    "        write_df.loc[zero_mask, new_column] = 0\n",
    "        write_df.loc[negative_mask, new_column] = math_func(df[column], neg=True)\n",
    "        \n",
    "        maxes[new_column] = write_df[new_column].abs().max()\n",
    "        write_df[new_column] = write_df[new_column]/ write_df[new_column].abs().max()\n",
    "        \n",
    "        \n",
    "        \n",
    "    return write_df, maxes\n",
    "\n",
    "\n",
    "\n",
    "def moving_avg(data, k=0.9):\n",
    "    ema = data[0]\n",
    "    vals = [data[0]]\n",
    "    \n",
    "    for point in data[1:]:\n",
    "        ema = ema*k + point*(1-k)\n",
    "        vals.append(ema)\n",
    "        \n",
    "    return vals\n",
    "        \n",
    "\n",
    "def add_moving_avgs(df, moving_avgs, varnames):\n",
    "    for k in moving_avgs:\n",
    "        for varname in varnames:\n",
    "            df[str(k).replace('.', '_')+'_ema_'+varname] =\\\n",
    "                df[varname].ewm(span=k).mean() #moving_avg(df[varname], k) #\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_data(df, columns, x='date'): \n",
    "    plt = px.line(\n",
    "        df,\n",
    "        x=x,\n",
    "        y=columns\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [0, 100, 1000, 10000], 'b': [0, -100, -1000, -10000]})\n",
    "\n",
    "mod_df, _ = math_func_scaling(\n",
    "    df, \n",
    "    scale_columns=['a', 'b'], \n",
    "    prefix='', \n",
    "    write_to_new_df=True,\n",
    "    math_func='sqrt'\n",
    ")\n",
    "\n",
    "maxes_1 = pd.DataFrame({'a': [1], 'b': [1]})\n",
    "maxes_0 = pd.DataFrame({'a': [10000], 'b': [10000]})\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    mod_df[col] = mod_df[col] * maxes_1[col].values[0]\n",
    "    mod_df[col] = np.square(mod_df[col])* maxes_0[col].values[0]\n",
    "    \n",
    "mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d391a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 'year'\n",
    "min_statements = 2\n",
    "max_years=30\n",
    "\n",
    "download_statements = False \n",
    "\n",
    "path_prefix = '' #'D:/'\n",
    "data_path = f\"{path_prefix}data/statements/{period}/\"\n",
    "pkl_path='aggregation/sequences'\n",
    "\n",
    "exchanges = ['NYSE']#[NASDAQ', 'EURONEXT' ,'NYSE', 'LSE']\n",
    "\n",
    "prefixes =  {'NASDAQ':'', 'EURONEXT':'', 'NYSE':'', 'LSE':''}\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "print(prefixes[exchanges[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_statements:\n",
    "    download_statement_data_for_exchanges(\n",
    "        exchanges, period, min_statements, data_path, prefix=prefixes[exchanges[0]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc404c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'symbol', 'year', 'years_since_public', #date', \n",
    "    'sector', 'country', 'industry', 'isEtf',\n",
    "    'exchange', 'isAdr', 'isFund', 'currency',\n",
    "    'reportedCurrency', 'isActivelyTrading',\n",
    "]\n",
    "\n",
    "deleted_cols = ['reportedCurrency', 'fullTimeEmployees']\n",
    "\n",
    "growth_cols = None\n",
    "raw_data_filename = 'raw_data.csv'\n",
    "\n",
    "\n",
    "moving_avgs = [10, 4] if period == 'year' else [50, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d636e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load = True\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "growth_cols = None\n",
    "numeric_cols = None\n",
    "\n",
    "if not load:\n",
    "    for path in tqdm(os.listdir(data_path)[1:]):\n",
    "        df = pd.read_csv(data_path+path, index_col=0)\n",
    "        \n",
    "        if growth_cols is None:\n",
    "            growth_cols = list(df.columns[df.columns.str.contains('growth')])\n",
    "            numeric_cols = [\n",
    "                col for col in df.columns if col not in categorical_cols+growth_cols+['date']+deleted_cols\n",
    "            ]\n",
    "\n",
    "        if 'date' in df.columns:\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(\n",
    "                    df['date'], format=\"%Y-%m-%d\"\n",
    "                )\n",
    "                df = df.sort_values(by='date', axis=0).reset_index(drop=True)\n",
    "\n",
    "                df['year'] = df['date'].dt.year\n",
    "                df['years_since_public'] = df.index\n",
    "\n",
    "                ysp = list(df.index)\n",
    "\n",
    "                if max(ysp) < 50:\n",
    "                    df = df.drop(deleted_cols, axis=1)\n",
    "\n",
    "                    df = add_moving_avgs(\n",
    "                        df,\n",
    "                        moving_avgs,\n",
    "                        numeric_cols\n",
    "                    )\n",
    "\n",
    "                    df['next_year_revenue'] = df['revenue'].shift(-1)\n",
    "                    df['next_year_freeCashFlow'] = df['freeCashFlow'].shift(-1)\n",
    "\n",
    "                    raw_data = raw_data.append(df, ignore_index=True)\n",
    "                else:\n",
    "                    print(df)\n",
    "            except:\n",
    "                print(str(e))\n",
    "                print(f'failed for {path}')\n",
    "        else:\n",
    "            print(\"Incorrect data for \", path)\n",
    "\n",
    "    raw_data.to_csv(data_path+raw_data_filename)\n",
    "    \n",
    "else:\n",
    "    df = pd.read_csv(data_path+os.listdir(data_path)[1], index_col=0)\n",
    "\n",
    "    growth_cols = list(df.columns[df.columns.str.contains('growth')])\n",
    "    numeric_cols = [\n",
    "        col for col in df.columns if col not in categorical_cols+growth_cols+['date']+deleted_cols\n",
    "    ]\n",
    "    raw_data = pd.read_csv(data_path+raw_data_filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d433962",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in os.listdir(data_path) if 'AAPL' in col] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec762d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in categorical_cols if col not in deleted_cols]\n",
    "\n",
    "column_dict = {\n",
    "    'growth': growth_cols, \n",
    "    'categorical': [col for col in categorical_cols if col[:2] != 'is'],\n",
    "    'bool': [col for col in categorical_cols if col[:2] == 'is'],\n",
    "    'moving_avgs': list(raw_data.columns[raw_data.columns.str.contains('_ema_')]),\n",
    "    'targets': list(raw_data.columns[raw_data.columns.str.contains('next_year')]),\n",
    "    'hash_encoded':  ['sector', 'country', 'industry', 'exchange'],\n",
    "    'one_hot_encoded': ['currency'],\n",
    "    'date_categoricals': ['year', 'years_since_public', 'date'],\n",
    "    'numeric': numeric_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eb46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for varname in categorical_cols:\n",
    "    try:\n",
    "        print(varname, len(raw_data[varname].value_counts()))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns[~all_data.columns.str.contains('next_year_revenue|next_year_freeCashFlow')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_data))\n",
    "billion = 1000000000\n",
    "\n",
    "all_data = raw_data[raw_data['currency'].isin(['USD', 'GBp', 'EUR', 'CAD'])]\n",
    "\n",
    "big_tickers = (\n",
    "    list(all_data[all_data['revenue'] > billion*1000]['symbol'].unique()) +\n",
    "    list(all_data[all_data['freeCashFlow'] > billion*1000]['symbol'].unique())\n",
    ")\n",
    "\n",
    "for ticker in big_tickers:\n",
    "    all_data = all_data[all_data['symbol'] != ticker]\n",
    "\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "print(len(all_data))\n",
    "\n",
    "\n",
    "#### TARGETS #####\n",
    "for col in column_dict['targets']:\n",
    "    all_data[col+'_bn_scale_tg'] =  all_data[col]/billion\n",
    "    \n",
    "    \n",
    "all_data, log_maxes = math_func_scaling(\n",
    "    all_data, \n",
    "    column_dict['targets'], \n",
    "    write_to_new_df=False,\n",
    "    prefix='_log_scale_tg',\n",
    "    math_func='log'\n",
    ")\n",
    "\n",
    "all_data, sqrt_maxes = math_func_scaling(\n",
    "    all_data, \n",
    "    column_dict['targets'], \n",
    "    write_to_new_df=False,\n",
    "    prefix='_sqrt_scale_tg',\n",
    "    math_func='sqrt'\n",
    ")\n",
    "\n",
    "log_maxes.to_csv('data/log_maxes.csv')\n",
    "sqrt_maxes.to_csv('data/sqrt_maxes.csv')\n",
    "\n",
    "column_dict['bn_scaled_targets'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_bn_scale_tg')])\n",
    "\n",
    "column_dict['log_scaled_targets'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_log_scale_tg')])\n",
    "\n",
    "column_dict['sqrt_scaled_targets'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_sqrt_scale_tg')])\n",
    "\n",
    "\n",
    "##### INPUTS #####\n",
    "for col in numeric_cols+column_dict['moving_avgs']:\n",
    "    all_data[col+'_bn_scale_inputs'] =  all_data[col]/billion\n",
    "\n",
    "all_data, _ = math_func_scaling(\n",
    "    all_data, \n",
    "    numeric_cols+column_dict['moving_avgs'], \n",
    "    write_to_new_df=False,\n",
    "    prefix='_log_scale_inputs',\n",
    "    math_func='log'\n",
    ")\n",
    "\n",
    "all_data, _ = math_func_scaling(\n",
    "    all_data, \n",
    "    numeric_cols+column_dict['moving_avgs'], \n",
    "    write_to_new_df=False,\n",
    "    prefix='_sqrt_scale_inputs',\n",
    "    math_func='sqrt'\n",
    ")\n",
    "\n",
    "\n",
    "all_data, _ = add_scaled_vars(\n",
    "    all_data,\n",
    "    ['year', 'years_since_public'],\n",
    "    write_to_new_df=False,\n",
    "    prefix='_scaled_years'\n",
    ")\n",
    "\n",
    "all_data = one_hot_encoding(\n",
    "    all_data, \n",
    "    ['currency'], \n",
    "    concat=True\n",
    ") \n",
    "\n",
    "\n",
    "all_data, hash_encoding_columns = hash_encode_gt_features(\n",
    "    all_data, \n",
    "    column_dict['hash_encoded'], \n",
    "    concat=True, \n",
    "    num_encode_dimensions=16, \n",
    "    hashing_method='md5'\n",
    ")\n",
    "\n",
    "\n",
    "column_dict['bn_scaled_inputs'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_bn_scale_inputs')])\n",
    "\n",
    "column_dict['log_scaled_inputs'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_log_scale_inputs')])\n",
    "\n",
    "column_dict['sqrt_scaled_inputs'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_sqrt_scale_inputs')])\n",
    "\n",
    "column_dict['scaled_years'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('_scaled_years')])\n",
    "\n",
    "column_dict['one_hot_encoded_columns'] = \\\n",
    "    list(all_data.columns[all_data.columns.str.contains('encoded')])\n",
    "\n",
    "column_dict['hash_encoded_columns'] = hash_encoding_columns \n",
    "\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "# mask out next year with no targets\n",
    "next_year_mask = all_data['next_year_revenue'].isna()\n",
    "next_year = all_data[next_year_mask].reset_index(drop=True)\n",
    "all_data = all_data[~next_year_mask].reset_index(drop=True)\n",
    "\n",
    "next_year.to_csv('data/next_year.csv')\n",
    "all_data.to_csv('data/all.csv')\n",
    "\n",
    "with open('data/column_dict.json', 'w') as ff:\n",
    "    json.dump(column_dict, ff) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NANs\n",
    "for index, val in all_data.isna().sum().iteritems():\n",
    "    if val:\n",
    "        print(index, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7463b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, val in column_dict.items():\n",
    "    print(key)\n",
    "    print('\\n\\t', '\\n\\t'.join(val), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe43fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = all_data[all_data['symbol'] == 'AAPL']\n",
    "fig = px.scatter(\n",
    "    dt, \n",
    "    x=\"year\", \n",
    "    y=[\n",
    "        'next_year_freeCashFlow_bn_scale_tg', \n",
    "        'next_year_freeCashFlow_log_scale_tg',\n",
    "        'freeCashFlow_bn_scale_inputs',\n",
    "        'freeCashFlow_log_scale_inputs'\n",
    "    ], \n",
    "    #color=\"species\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846be8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data/all.csv')\n",
    "next_year = pd.read_csv('data/next_year.csv')\n",
    "column_dict = json.load(open('data/column_dict.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in all_data.columns[all_data.columns.str.contains('ema')]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63c215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Possible Sections: \\n \", '\\n  '.join(list(column_dict.keys())), \"\\n\\n\")\n",
    "\n",
    "remove_nan = True\n",
    "cat = True\n",
    "just_free_cash_flow_target = True\n",
    "remove_revenue_above_1bn = True\n",
    "\n",
    "remove_ema = 10\n",
    "reg_type = 'bn'\n",
    "\n",
    "specific_columns = []\n",
    "segments = ['growth', 'bool'] \n",
    "\n",
    "\n",
    "affix = reg_type+'_cat' if cat else reg_type\n",
    "        \n",
    "\n",
    "segments = segments + [\n",
    "    'one_hot_encoded_columns',\n",
    "    'hash_encoded_columns',\n",
    "    'scaled_years', \n",
    "] if not cat else segments + ['categorical'] \n",
    "\n",
    "\n",
    "if reg_type == 'log':\n",
    "    segments = segments + ['log_scaled_inputs'] \n",
    "    targets = column_dict['log_scaled_targets']\n",
    "    \n",
    "elif reg_type == 'sqrt':\n",
    "    segments = segments + ['sqrt_scaled_inputs']\n",
    "    targets = column_dict['sqrt_scaled_targets']\n",
    "    \n",
    "else:\n",
    "    segments = segments + ['bn_scaled_inputs']\n",
    "    targets = column_dict['bn_scaled_targets']\n",
    "    \n",
    "\n",
    "input_cols = []\n",
    "for segment in segments:\n",
    "    input_cols.extend(column_dict[segment])\n",
    "\n",
    "input_cols.extend(specific_columns)\n",
    "\n",
    "if remove_ema:\n",
    "    affix += f'_NO{remove_ema}EMA'\n",
    "    input_cols = [\n",
    "        col for col in input_cols \n",
    "        if str(remove_ema)+'_ema' not in col\n",
    "    ]\n",
    "    \n",
    "\n",
    "print(len(all_data))\n",
    "\n",
    "if remove_nan:\n",
    "    mod_data = copy.deepcopy(\n",
    "        all_data[~all_data[input_cols+targets].isnull().any(axis=1)]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    mod_next_year = copy.deepcopy(\n",
    "        next_year[~next_year[input_cols].isnull().any(axis=1)]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    mod_data = copy.deepcopy(all_data)\n",
    "    mod_next_year = copy.deepcopy(next_year)\n",
    "    affix += '_nans'\n",
    "    \n",
    "if just_free_cash_flow_target:\n",
    "    affix += \"_JFCF\"\n",
    "    targets = [col for col in targets if 'freeCashFlow' in col]\n",
    "    \n",
    "if remove_revenue_above_1bn:\n",
    "    mod_data = mod_data[mod_data['4_ema_revenue'] < 1000000000]\n",
    "    mod_next_year = mod_next_year[mod_next_year['4_ema_revenue'] < 1000000000]\n",
    "    affix += '_UP21bn'\n",
    "    \n",
    "print(len(mod_data))\n",
    "print(affix, \"\\n\", segments, \"\\n\", targets)\n",
    "\n",
    "target_years = [2006, 2012, 2018]\n",
    "\n",
    "to_float_cols = [col for col in input_cols if col not in column_dict['categorical']]\n",
    "mod_data[to_float_cols] = mod_data[to_float_cols].astype(np.float32)\n",
    "mod_next_year[to_float_cols] = mod_next_year[to_float_cols].astype(np.float32)\n",
    "\n",
    "datasets = {\n",
    "    'train': mod_data[mod_data['year'].isin(\n",
    "        [\n",
    "            year for year in mod_data['year'].unique() \n",
    "             if year not in target_years+[2019, 2020]\n",
    "        ]\n",
    "    )],\n",
    "    'test': mod_data[mod_data['year'].isin(target_years)],\n",
    "    'val': mod_data[mod_data['year'] == 2020],\n",
    "    'next_year': mod_next_year\n",
    "}\n",
    "\n",
    "for key, data in datasets.items():\n",
    "    data[input_cols+['symbol']].to_csv(\n",
    "        f'data/performance_forecasting/{key}_input_{affix}.csv'\n",
    "    )\n",
    "    data[targets+['symbol']].to_csv(\n",
    "        f'data/performance_forecasting/{key}_output_{affix}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30724cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ds in datasets.items():\n",
    "    print(key, len(ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
