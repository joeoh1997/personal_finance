{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0d240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from performance_forecasting.data_creator import download_statement_data_for_exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7303d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_predictions(regressor, input_data, targets, sess, batch_size, ops, dtype=np.float32):\n",
    "    preds = None\n",
    "    op_results = {str(i): [] for i in range(len(ops))}\n",
    "\n",
    "    for j in range(0, len(targets), batch_size):              #  \n",
    "        op_result = sess.run(\n",
    "            ops,\n",
    "            {\n",
    "                regressor.x_placeholder: input_data[j: j+batch_size],\n",
    "                regressor.y_placeholder: targets[j: j+batch_size]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        for i, res in enumerate(op_result):\n",
    "            op_results[str(i)].append(res)\n",
    "    \n",
    "    mod_results = []\n",
    "    for i in range(len(ops)):\n",
    "        if type(op_results[str(i)][0]) == dtype:\n",
    "            mod_results.append(op_results[str(i)])\n",
    "        else:\n",
    "            mod_results.append(np.concatenate(op_results[str(i)], axis=0))\n",
    "        \n",
    "    return mod_results\n",
    "            \n",
    "\n",
    "def plot_predictions(regressor, input_data, targets, sess, batch_size, ops, dtype=np.float32):\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    losses, preds = get_predictions(\n",
    "        regressor, input_data, targets, sess, batch_size, ops, dtype\n",
    "    )\n",
    "    print(targets)\n",
    "    results['pred_0'] = preds[:, 0]\n",
    "    results['pred_1'] = preds[:, 1]\n",
    "    results['label_0'] = targets[:, 0]\n",
    "    results['label_1'] = targets[:, 1]\n",
    "\n",
    "    results = results.sort_values(by='label_0')\n",
    "\n",
    "    plt = px.scatter(\n",
    "        results,\n",
    "        x='pred_0',\n",
    "        y='label_0'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    plt = px.scatter(\n",
    "        results,\n",
    "        x='pred_1',\n",
    "        y='label_1'\n",
    "    )\n",
    "    plt.show()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94827249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(sess, save_dir, regressor, save_info, scores):\n",
    "    shutil.rmtree(save_dir)\n",
    "\n",
    "    tf.compat.v1.saved_model.simple_save(\n",
    "        sess, save_dir,\n",
    "        inputs=regressor.save_params['input'],\n",
    "        outputs=regressor.save_params['output']\n",
    "    )\n",
    "    \n",
    "    return save_info.append({\n",
    "        'test': scores[0],\n",
    "        'val': scores[1],\n",
    "        'val_test': scores[2]\n",
    "    }, ignore_index=True)\n",
    "           \n",
    "    \n",
    "# layer_sizes, activations, dropouts, use_batch_bools,\n",
    "def train_model_with_settings(\n",
    "    regressor,\n",
    "    params,\n",
    "    iterations,\n",
    "    plot=False,\n",
    "    savedir='performance_forecasting/models/',\n",
    "    print_losses=True,\n",
    "    save_options=None\n",
    "):    \n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=2)\n",
    "    save_info = pd.DataFrame(columns=['best_val', 'best_test'], data=np.zeros([1, 2])) \n",
    "    \n",
    "    runs = []\n",
    "    \n",
    "    for path in os.listdir(savedir):\n",
    "        try:\n",
    "            runs.append(int(path))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    save_dir = savedir + str(max(runs) + 1) + \"/\"\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "    params['activations'] = str(params['activations'])\n",
    "    \n",
    "    filename = save_dir+\"model\" \n",
    "    pickle.dump(params,  open(filename+\".pkl\", 'wb'))\n",
    "    \n",
    "    save_dir =  save_dir + 'tf/'\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        epoch_loss = 0 \n",
    "        mean_test_losses = []\n",
    "        mean_epoch_losses = []\n",
    "        mean_val_losses = []\n",
    "        min_val_test_losses = [] #np.inf\n",
    "\n",
    "        for i in tqdm(range(iterations)):\n",
    "            indexes = np.random.randint(0, high=len(regressor.y_train), size=batch_size)\n",
    "\n",
    "            loss_val, _, yy_pred = sess.run(\n",
    "                [regressor.loss, regressor.optim_op, regressor.y_pred],\n",
    "                {\n",
    "                    regressor.x_placeholder: regressor.X_train[indexes],\n",
    "                    regressor.y_placeholder: regressor.y_train[indexes]\n",
    "                }\n",
    "            )\n",
    "            #print(yy_pred)\n",
    "\n",
    "            epoch_loss += loss_val\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                train_loss = epoch_loss/500\n",
    "                mean_epoch_losses.append(train_loss)\n",
    "                epoch_loss = 0\n",
    "\n",
    "                if plot and i % 10000 == 0:\n",
    "                    losses = plot_predictions(\n",
    "                        regressor, regressor.X_test, regressor.y_test,\n",
    "                        sess, batch_size,\n",
    "                        [regressor.abs_loss, regressor.y_pred],\n",
    "                        dtype=np.float32\n",
    "                    )\n",
    "                    val_losses = plot_predictions(\n",
    "                        regressor, regressor.X_val, regressor.y_val, \n",
    "                        sess, batch_size, \n",
    "                        [regressor.abs_loss, regressor.y_pred],\n",
    "                        dtype=np.float32\n",
    "                    )\n",
    "                else:\n",
    "                    losses = np.array(get_predictions(\n",
    "                        regressor, regressor.X_test, regressor.y_test,\n",
    "                        sess, batch_size,\n",
    "                        [regressor.abs_loss],\n",
    "                        dtype=np.float32\n",
    "                    )[0])\n",
    "                    val_losses = np.array(get_predictions(\n",
    "                        regressor, regressor.X_val, regressor.y_val,\n",
    "                        sess, batch_size,\n",
    "                        [regressor.abs_loss],\n",
    "                        dtype=np.float32\n",
    "                    )[0])\n",
    "                    \n",
    "                #print(val_losses)\n",
    "\n",
    "                val_loss, test_loss = np.mean(val_losses), np.mean(losses)\n",
    "                \n",
    "                #print(val_losses.shape, losses.shape)\n",
    "                \n",
    "                val_test_loss = np.mean(\n",
    "                    np.concatenate([val_losses, losses], axis=0)\n",
    "                )\n",
    "\n",
    "                if print_losses:\n",
    "                    print(f\"Losses: train={train_loss}, test={test_loss}, val={val_loss}, val_test={val_test_loss}\")\n",
    "\n",
    "\n",
    "                if len(mean_test_losses) > 0 and save_options['save']:\n",
    "\n",
    "                    if save_options['save_by'] == 'test' and test_loss < np.amin(mean_test_losses):\n",
    "                        print(f'New best test loss ({test_loss}) saving model..')\n",
    "                        save_info = save_model(\n",
    "                            sess, save_dir, regressor, save_info,\n",
    "                            [test_loss, val_loss, val_test_loss]\n",
    "                        )\n",
    "\n",
    "\n",
    "                    elif save_options['save_by'] == 'val' and val_loss < np.amin(mean_val_losses):\n",
    "                        print(f'New best val loss ({val_loss}) saving model..')\n",
    "                        save_info = save_model(\n",
    "                            sess, save_dir, regressor, save_info,\n",
    "                            [test_loss, val_loss, val_test_loss]\n",
    "                        )\n",
    "                        \n",
    "                        \n",
    "                    elif save_options['save_by'] == 'both' and val_test_loss < np.amin(min_val_test_losses):\n",
    "                        print(f'New best val_test loss ({val_test_loss}) saving model..')\n",
    "                        save_info = save_model(\n",
    "                            sess, save_dir, regressor, save_info,\n",
    "                            [test_loss, val_loss, val_test_loss]\n",
    "                        )\n",
    "                        min_val_test_loss = val_test_loss\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                mean_val_losses.append(val_loss)\n",
    "                mean_test_losses.append(test_loss)\n",
    "                min_val_test_losses.append(val_test_loss)\n",
    "                \n",
    "        save_info.to_csv(save_dir+'best_model_info.csv')\n",
    "        test_min_indx = np.argmin(mean_test_losses)\n",
    "        return (\n",
    "            mean_epoch_losses[test_min_indx], \n",
    "            mean_test_losses[test_min_indx], \n",
    "            mean_val_losses[test_min_indx],\n",
    "            min_val_test_losses[test_min_indx]\n",
    "        )\n",
    "        #return np.min(mean_epoch_losses), np.min(mean_test_losses), np.min(mean_val_losses), np.min(min_val_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53a241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_diff(t0, t1):\n",
    "    return tf.compat.v1.reduce_mean(tf.compat.v1.abs(t0 - t1))\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "class TF_vOne_Regressor():\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        dtype,\n",
    "        seed\n",
    "    ):\n",
    "        \n",
    "        dataset_names = ['train', 'test', 'val']\n",
    "        dataset = {}\n",
    "        dataset_affix = params['dataset_affix']\n",
    "\n",
    "        for key in dataset_names:\n",
    "            dataset['x_'+key] = pd.read_csv(\n",
    "                f'data/performance_forecasting/{key}_input{dataset_affix}.csv', \n",
    "                index_col=0\n",
    "            ).drop('symbol', axis=1).values\n",
    "            dataset['y_'+key] = pd.read_csv(\n",
    "                f'data/performance_forecasting/{key}_output{dataset_affix}.csv', \n",
    "                index_col=0\n",
    "            ).drop('symbol', axis=1).values\n",
    "\n",
    "        self.X_train, self.y_train = dataset['x_train'], dataset['y_train']\n",
    "        self.X_test, self.y_test = dataset['x_test'], dataset['y_test']\n",
    "        self.X_val, self.y_val = dataset['x_val'], dataset['y_val']\n",
    "        \n",
    "        params['in_shape'] = dataset['x_train'].shape[1]\n",
    "        params['out_shape'] = dataset['y_train'].shape[1]\n",
    "    \n",
    "        self.x_placeholder = tf.compat.v1.placeholder(\n",
    "            dtype=dtype, shape=(None, params['in_shape']), name='x_placeholder_saved'\n",
    "        )\n",
    "        self.y_placeholder = tf.compat.v1.placeholder(\n",
    "            dtype=dtype, shape=(None, params['out_shape']), name='y_placeholder_saved'\n",
    "        )\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal(seed=seed)\n",
    "\n",
    "        self.layer_weights = []\n",
    "        self.build_weights(params, dtype)\n",
    "\n",
    "        self.layer = self.x_placeholder\n",
    "        self.build_layers(params)\n",
    "        \n",
    "        self.y_pred = tf.identity(tf.linalg.matmul(\n",
    "            self.layer, self.layer_weights[-1]#, name='y_pred'\n",
    "        ), name=\"prediction\")\n",
    "\n",
    "        optim = params['optimizer'](\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "\n",
    "        self.loss = params['loss_function'](self.y_placeholder, self.y_pred)\n",
    "        self.abs_loss = tf.compat.v1.losses.absolute_difference(self.y_placeholder, self.y_pred)\n",
    "        self.optim_op = optim.minimize(self.loss)\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.save_params = {\n",
    "            \"input\": {\n",
    "                \"x_placeholder_saved\": self.x_placeholder,\n",
    "                \"y_placeholder_saved\": self.y_placeholder\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"prediction\": self.y_pred\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def build_weights(self, params, dtype):\n",
    "        last_out_size = params['in_shape']\n",
    "\n",
    "        for i, size in enumerate(list(params['layer_sizes'])+[params['out_shape']]):\n",
    "            self.layer_weights.append(tf.Variable(\n",
    "                self.initializer([last_out_size, size], dtype=dtype),\n",
    "                name='l'+str(i+1),\n",
    "                dtype=dtype\n",
    "            ))\n",
    "            last_out_size = size \n",
    "\n",
    "\n",
    "    def build_layers(self, params):\n",
    "        for i, activation in enumerate(params['activations']):\n",
    "            self.layer = tf.linalg.matmul(self.layer, self.layer_weights[i])\n",
    "\n",
    "            if params['use_batch_bools'][i]:\n",
    "                self.layer = tf.compat.v1.layers.batch_normalization(self.layer)\n",
    "\n",
    "            self.layer = params['activations'][i](self.layer)\n",
    "            self.layer = tf.compat.v1.layers.dropout(self.layer, rate=params['activations'][i])\n",
    "            print(self.layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b3f16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TF static tensor graph build (tf v1)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "neural_width_scaling = 55 # default was 60\n",
    "batch_size = 50\n",
    "seed = 1221\n",
    "dtype = tf.float32\n",
    "\n",
    "\n",
    "optim_functions = [\n",
    "    tf.compat.v1.train.RMSPropOptimizer,\n",
    "    tf.compat.v1.train.AdamOptimizer,\n",
    "    tf.compat.v1.train.AdagradOptimizer\n",
    "]\n",
    "\n",
    "loss_functions = [\n",
    "    tf.compat.v1.losses.absolute_difference,\n",
    "    tf.compat.v1.losses.mean_squared_error,\n",
    "    tf.compat.v1.losses.huber_loss\n",
    "]\n",
    "\n",
    "activation_functions = [\n",
    "    tf.nn.relu6,\n",
    "    tf.nn.relu,\n",
    "    tf.nn.gelu,\n",
    "    tf.nn.tanh,\n",
    "    tf.nn.sigmoid,\n",
    "    linear\n",
    "]\n",
    "\n",
    "learning_rates = [0.1, 0.05, 0.01, 0.001, 0.0005]\n",
    "\n",
    "dataset_affix = \"bn_NO10EMA_UP21bn\"\n",
    "runs_filename = f\"performance_forecasting/runs/runs_{dataset_affix}.csv\"\n",
    "\n",
    "num_searches = 10000\n",
    "search_run = False\n",
    "iterations = 30000 if search_run else 200000\n",
    "run_model = 1\n",
    "\n",
    "\n",
    "if os.path.isfile(runs_filename):\n",
    "    runs_df = pd.read_csv(runs_filename, index_col=0)\n",
    "else:\n",
    "    runs_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "if search_run:\n",
    "    for _ in range(num_searches):\n",
    "        num_layers = random.randint(0, 7) # 8)\n",
    "        params = {\n",
    "            'layer_sizes': np.clip(np.clip(\n",
    "                    np.random.f(10, 50, size=num_layers), 0, 5\n",
    "                )*neural_width_scaling, 3, 300).astype(int),\n",
    "            'activations': np.random.choice(activation_functions, size=num_layers, replace=True),\n",
    "            'dropouts': np.random.uniform(0, 0.9, num_layers),\n",
    "            'use_batch_bools': np.random.choice([True, False], size=num_layers, replace=True),\n",
    "            'loss_function': random.sample(loss_functions, 1)[0],\n",
    "            'optimizer': random.sample(optim_functions, 1)[0],\n",
    "            'learning_rate': random.sample(learning_rates, 1)[0],\n",
    "            'dataset_affix': '_'+dataset_affix\n",
    "        }\n",
    "        \n",
    "        regressor = TF_vOne_Regressor(\n",
    "            params,\n",
    "            dtype=dtype,\n",
    "            seed=1221\n",
    "        )\n",
    "\n",
    "        train_loss, test_loss, val_loss, val_test_loss = train_model_with_settings(\n",
    "            regressor, params, iterations, \n",
    "            plot=False, print_losses=False,\n",
    "            save_options={\n",
    "                'save_by': 'both', 'save': False\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss = {train_loss}, Test loss = {test_loss},\"\n",
    "            f\"Val loss = {val_loss}, Val Test Loss {val_test_loss}\"\n",
    "        )\n",
    "\n",
    "        #layers = {key: ', '.join(val.astype(str)) for key, val in layers.items()}\n",
    "        \n",
    "        for key, val in params.items():\n",
    "            if key in ['layer_sizes', 'activations', 'dropouts', 'use_batch_bools']:\n",
    "                params[key] = ', '.join(val.astype(str))\n",
    "\n",
    "        runs_df = runs_df.append(\n",
    "            {**params, **{\n",
    "                'train_loss':train_loss, 'test_loss': test_loss, \n",
    "                'val_loss': val_loss, 'val_test_loss': val_test_loss\n",
    "            }},\n",
    "            ignore_index = True\n",
    "        )\n",
    "        runs_df.sort_values(by='test_loss').to_csv(runs_filename)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    if run_model == 0:    ## best (didnt search that long) bilion no 10 ema\n",
    "        params = {\n",
    "            'layer_sizes': [55, 37],\n",
    "            'activations': [tf.nn.gelu, tf.nn.relu],\n",
    "            'dropouts': [\n",
    "                0.807198580715569, 0.26161063031293397\n",
    "            ],\n",
    "            'use_batch_bools': [False, True],\n",
    "            'loss_function': tf.compat.v1.losses.absolute_difference,\n",
    "            'optimizer': tf.compat.v1.train.RMSPropOptimizer,\n",
    "            'learning_rate': 0.001,\n",
    "            'dataset_affix': '_bn_NO10EMA_UP21bn'\n",
    "        }\n",
    "    \n",
    "\n",
    "        regressor = TF_vOne_Regressor(\n",
    "            params,\n",
    "            dtype=dtype,\n",
    "            seed=1221\n",
    "        )\n",
    "\n",
    "        train_loss, test_loss, val_loss = train_model_with_settings(\n",
    "            regressor, params, iterations, plot=False, save_options={\n",
    "                'save_by': 'test', 'save': True\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    elif run_model == 1:   ## 2nd best log (could do a better search)\n",
    "        params = {\n",
    "            'layer_sizes': [98, 47, 8, 125, 28],\n",
    "            'activations': [tf.nn.sigmoid, tf.nn.gelu, linear, linear, tf.nn.tanh],\n",
    "            'dropouts': [\n",
    "                0.1995318542098317, 0.8236512364029288, 0.6098110059963907, \n",
    "                0.6208227090084751, 0.8886921709358674\n",
    "            ],\n",
    "            'use_batch_bools': [False, True, True, True, True],\n",
    "            'loss_function': tf.compat.v1.losses.absolute_difference,\n",
    "            'optimizer': tf.compat.v1.train.RMSPropOptimizer,\n",
    "            'learning_rate': 0.001,\n",
    "            'dataset_affix': '_sqrt_NO10EMA_UP21bn'\n",
    "        }\n",
    "\n",
    "        regressor = TF_vOne_Regressor(\n",
    "            params,\n",
    "            dtype=dtype,\n",
    "            seed=1221\n",
    "        )\n",
    "\n",
    "        train_loss, test_loss, val_loss = train_model_with_settings(\n",
    "            regressor, params, iterations, plot=True, save_options={\n",
    "                'save_by': 'val', 'save': True\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "    print(f\"Train Loss = {train_loss}, Test loss = {test_loss}, Val loss = {val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
